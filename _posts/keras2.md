# Keras : Chapter 2
---


**í…ì„œ : Tensor**
í…ì„œí”Œë¡œìš°ì—ì„œ ìƒìˆ˜ í…ì„œì—ëŠ” ê°’ì„ í• ë‹¹í•  ìˆ˜ ì—†ë‹¤. ë”°ë¼ì„œ ë³€ìˆ˜ í…ì„œë¥¼ ì‚¬ìš©í•œë‹¤.


```python
#!pip install tensorflow
import tensorflow as tf

# ë³€ìˆ˜ í…ì„œ
v = tf.Variable(initial_value=tf.random.normal(shape=(3,1)))
print(v)
v.assign(tf.ones((3,1)))
print(v)
```

    <tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy=
    array([[-1.020172 ],
           [ 0.6342499],
           [-1.4028376]], dtype=float32)>
    <tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy=
    array([[1.],
           [1.],
           [1.]], dtype=float32)>
    


**ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ : Back Propagation**
ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì€ ì‹ ê²½ë§ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ê°’ì„ ê³„ì‚°í•˜ëŠ” ë° ë¯¸ì ë¶„ì˜ ì—°ì‡„ ë²•ì¹™ì„ ì ìš©í•˜ëŠ” ê²ƒ.
ì—°ì‡„ ë²•ì¹™ì„ ê³„ì‚° ê·¸ë˜í”„ì— ì ìš©í•œ ê²ƒì´ë‹¤.

- Tensorflow ì˜ Gradient Tape(Computational Graph) API



```python
import tensorflow as tf

x = tf.Variable(0,) #ì´ˆê¹ƒê°’ 0ìœ¼ë¡œ ìŠ¤ì¹¼ë¼ ë³€ìˆ˜ìƒì„±
with tf.GradientTape() as tape: #Gradient tape ë¸”ë¡ ìƒì„±
    y = 2*x + 3 #í…ì„œ ì—°ì‚°
    grad_of_y_yrt_x = tape.gradient(y, x) #ë³€ìˆ˜ xì— ëŒ€í•œ ì¶œë ¥ yì˜ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    
    
#ë‹¤ì°¨ì› í…ì„œì˜ ê·¸ë˜ë””ì–¸íŠ¸
x = tf.Variable(tf.zeros(2,2)) #(2,2) í¬ê¸°ì˜ 0ìœ¼ë¡œ ì±„ì›Œì§„ ë³€ìˆ˜ìƒì„±
with tf.GradientTape() as tape: #Gradient tape ë¸”ë¡ ìƒì„±
    y = 2*x + 3 #í…ì„œ ì—°ì‚°
    grad_of_y_yrt_x = tape.gradient(y, x)
```

grad_of_y_yrt_x ëŠ” xì™€ í¬ê¸°ê°€ ê°™ì€ (2,2) í¬ê¸°ì˜ í…ì„œì´ë‹¤. x = [[0,0],[0,0]] ì¼ ë•Œ, y=2*x+3ì˜ ê¸°ìš¸ê¸°(ê³¡ë¥ )ì„ ë‚˜íƒ€ë‚¸ë‹¤.


```python
#ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ì˜ ê·¸ë˜ë””ì–¸íŠ¸
W = tf.Variable(tf.random.uniform((2,2)))
b = tf.Variable(tf.zeros((2,)))
x = tf.random.uniform((2,2))
with tf.GradientTape() as tape:
    y = tf.matmul(x,W) + b #ì ê³± í•¨ìˆ˜.
grad_of_y_yrt_W_and_b = tape.gradient(y, [W, b])
```

grad_of_y_yrt_x ëŠ” 2ê°œì˜ í…ì„œë¥¼ ë‹´ì€ ë¦¬ìŠ¤íŠ¸. ê° í…ì„œëŠ” W, bì™€ í¬ê¸°ê°€ ê°™ë‹¤.

## MNIST ì˜ˆì œ : Keras


```python
#tensorflow.keras.layersì—ì„œ Dense ë¶ˆëŸ¬ì˜¤ê¸°
#ë°ì´í„° ì…ë ¥
import numpy as np
from tensorflow.keras.datasets import mnist
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255
test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype("float32") / 255

# ëª¨ë¸ ì •ì˜
model = Sequential([
    Dense(512, activation="relu"),
    Dense(10, activation="softmax") #layers,Dense ê°€ ì•„ë‹Œ Dense ë¡œ ì‚¬ìš©
])

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

# ëª¨ë¸ í•™ìŠµ
model.fit(train_images, train_labels, epochs=5, batch_size=128)
```

    Epoch 1/5
    [1m469/469[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 4ms/step - accuracy: 0.8737 - loss: 0.4401
    Epoch 2/5
    [1m469/469[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 3ms/step - accuracy: 0.9662 - loss: 0.1177
    Epoch 3/5
    [1m469/469[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 3ms/step - accuracy: 0.9797 - loss: 0.0700
    Epoch 4/5
    [1m469/469[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 4ms/step - accuracy: 0.9852 - loss: 0.0503
    Epoch 5/5
    [1m469/469[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 4ms/step - accuracy: 0.9894 - loss: 0.0363
    




    <keras.src.callbacks.history.History at 0x12237b0ad30>




```python
#tensorflow.keras ì—ì„œ layers ë¶ˆëŸ¬ì˜¤ê¸°
import numpy as np
from tensorflow.keras.datasets import mnist
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers

#ì…ë ¥ ë°ì´í„° : ë°ì´í„° íƒ€ì… float32, í›ˆë ¨ ë°ì´í„° í¬ê¸°ëŠ” (60000, 784) í¬ê¸° 
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255
test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype("float32") / 255

#ëª¨ë¸ : 2ê°œì˜ Dense ì¸µì´ ì—°ê²°ë˜ì–´ ìˆê³ , ê° ì¸µì€ ê°€ì¤‘ì¹˜ í…ì„œë¥¼ í¬í•¨í•´ ì…ë ¥ ë°ì´í„°ì— ëŒ€í•œ í…ì„œ ì—°ì‚°ì„ ì ìš©í•œë‹¤.
#ê°€ì¤‘ì¹˜ í…ì„œëŠ” ëª¨ë¸ì´ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ê³³
model = keras.Sequential([
    layers.Dense(512, activation="relu"), 
    layers.Dense(10, activation="softmax")
])

# ëª¨ë¸ ì»´íŒŒì¼ : ëª¨ë¸í•™ìŠµì— í•„ìš”í•œ í•„ìˆ˜ì ì¸ ìš”ì†Œ ì •ì˜ ë° ì„¤ì •
model.compile(optimizer="rmsprop", #ì†ì‹¤í•¨ìˆ˜ë¥¼ ì¤„ì´ê¸°ìœ„í•´ ê°€ì¤‘ì¹˜ë¥¼ ì–´ë–»ê²Œ ì¡°ì ˆí• ì§€
              loss="sparse_categorical_crossentropy",#ëª¨ë¸ í•™ìŠµ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ê°’ ì°¨ì´ë¥¼ ì¸¡ì •
              metrics=["accuracy"])#ë¶„ë¥˜ ë¬¸ì œì´ë¯€ë¡œ ì •í™•ë„ë¥¼ ì‚¬ìš©í•´ ì„±ëŠ¥í™•ì¸

#ëª¨ë¸ í•™ìŠµ : í›ˆë ¨ ë°˜ë³µ
model.fit(train_images, train_labels, epochs=5, batch_size=128)
```

    Epoch 1/5
    [1m469/469[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 3ms/step - accuracy: 0.8739 - loss: 0.4450
    Epoch 2/5
    [1m469/469[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 3ms/step - accuracy: 0.9680 - loss: 0.1141
    Epoch 3/5
    [1m469/469[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 3ms/step - accuracy: 0.9788 - loss: 0.0693
    Epoch 4/5
    [1m469/469[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 3ms/step - accuracy: 0.9853 - loss: 0.0532
    Epoch 5/5
    [1m469/469[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 3ms/step - accuracy: 0.9890 - loss: 0.0358
    




    <keras.src.callbacks.history.History at 0x1223ba7e160>



# MNIST ì˜ˆì œ : Tensorflowë¡œ êµ¬í˜„í•˜ê¸°
---

- 1. ë‹¨ìˆœí•œ Dense í´ë˜ìŠ¤
Dense ì¸µì€ ì…ë ¥ ë³€í™˜ì„ êµ¬í˜„í•©ë‹ˆë‹¤.


ëª¨ë¸ íŒŒë¼ë¯¸í„° : W(Weight), b(bias)

Activation : ê° ì›ì†Œì— ì ìš©ë˜ëŠ” í•¨ìˆ˜.

output = activation(dot(W, input) + b)



```python
import tensorflow as tf

class NaiveDense:
    def __init__(self, input_size, output_size, activation):
        self.activation = activation

        w_shape = (input_size, output_size) #ëœë¤ê°’ìœ¼ë¡œ ì´ˆê¸°í™”ëœ (input_size, output_size)í¬ê¸°ì˜ í–‰ë ¬
        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)
        self.W = tf.Variable(w_initial_value)

        b_shape = (output_size,) #ë²¡í„°ì˜ í¬ê¸°ì§€ì •
        b_initial_value = tf.zeros(b_shape) #(output_size,) í¬ê¸°ì˜ 0ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë²¡í„°
        self.b = tf.Variable(b_initial_value)

    def __call__(self, inputs):
        return self.activation(tf.matmul(inputs, self.W) + self.b)

    @property
    def weights(self):
        return [self.W, self.b]
```

- 2. ë‹¨ìˆœí•œ Sequential í´ë˜ìŠ¤


```python
class NaiveSequential:
    def __init__(self, layers):
        self.layers = layers

    def __call__(self, inputs):
        x = inputs
        for layer in self.layers:
           x = layer(x)
        return x

    @property
    def weights(self):
       weights = []
       for layer in self.layers:
           weights += layer.weights
       return weights
```

- NaiveDense í´ë˜ìŠ¤ì™€ NaiveSequential í´ë˜ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬ ì¼€ë¼ìŠ¤ì™€ ìœ ì‚¬í•œ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.


```python
model = NaiveSequential([ #2
    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu), #1
    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax) #1
])
assert len(model.weights) == 4
```

- 3. ë°°ì¹˜ ì œë„ˆë ˆì´í„° : MNIST ë°ì´í„°ë¥¼ ë¯¸ë‹ˆ ë°°ì¹˜ë¡œ ìˆœíšŒí•˜ëŠ” ë°©ë²•


```python
import math

class BatchGenerator:
    def __init__(self, images, labels, batch_size=128):
        assert len(images) == len(labels)
        self.index = 0
        self.images = images
        self.labels = labels
        self.batch_size = batch_size
        self.num_batches = math.ceil(len(images) / batch_size)

    def next(self):
        images = self.images[self.index : self.index + self.batch_size]
        labels = self.labels[self.index : self.index + self.batch_size]
        self.index += self.batch_size
        return images, labels
```

- 4. í›ˆë ¨ ìŠ¤í… ì‹¤í–‰ì½”ë“œ


```python
#í›ˆë ¨ ìŠ¤í… ì‹¤í–‰ : ê°€ì¥ ì–´ë ¤ìš´ íŒŒíŠ¸
#í•œ ë°°ì¹˜ ë°ì´í„°ì—ì„œ ëª¨ë¸ì„ ì‹¤í–‰í•˜ê³  ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì¼

def one_training_step(model, images_batch, labels_batch):
    with tf.GradientTape() as tape: 
        # GradientTape ë¸”ë¡ ì•ˆì—ì„œ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê³„ì‚°
        #ì •ë°©í–¥ íŒ¨ìŠ¤ë¥¼ ì‹¤í–‰ 
        predictions = model(images_batch)
        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(
            labels_batch, predictions)
        average_loss = tf.reduce_mean(per_sample_losses)
    gradients = tape.gradient(average_loss, model.weights)
    #ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ì†ì‹¤ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°. Gradients ë¦¬ìŠ¤íŠ¸ì˜ ê° í•­ëª©ì€ model.weightsë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ê°€ì¤‘ì¹˜ì— ë§¤ì¹­ë¨
    update_weights(gradients, model.weights) #5
    #í•´ë‹¹ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ”í•¨ìˆ˜.
    return average_loss
```

- 5. update_weights ì •ì˜


```python
learning_rate = 1e-3

def update_weights(gradients, weights):
    for g, w in zip(gradients, weights):
        w.assign_sub(g * learning_rate) #tensorflow ë³€ìˆ˜ì˜ assign_sub ë§¤ì„œë“œëŠ” -= dhk ehddlf
```

ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ë‹¨ê³„ë¥¼ ìˆ˜ë™ìœ¼ë¡œ êµ¬í˜„í•˜ëŠ” ê²½ìš°ëŠ” ê±°ì˜ ì—†ê³ , Optimizerì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•œë‹¤.


```python
from tensorflow.keras import optimizers

optimizer = optimizers.SGD(learning_rate=1e-3)

def update_weights(gradients, weights):
    optimizer.apply_gradients(zip(gradients, weights))
```

- ì „ì²´ í›ˆë ¨ ë£¨í”„


```python
def fit(model, images, labels, epochs, batch_size=128):
    for epoch_counter in range(epochs):
        print(f"ì—í¬í¬ {epoch_counter}")
        batch_generator = BatchGenerator(images, labels) #3
        for batch_counter in range(batch_generator.num_batches):
            images_batch, labels_batch = batch_generator.next()
            loss = one_training_step(model, images_batch, labels_batch) #4
            if batch_counter % 100 == 0:
                print(f"{batch_counter}ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: {loss:.2f}")
```

- ëª¨ë¸ í•™ìŠµ í…ŒìŠ¤íŠ¸


```python
from tensorflow.keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255
test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype("float32") / 255

fit(model, train_images, train_labels, epochs=10, batch_size=128)
```

    ì—í¬í¬ 0
    0ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 4.74
    100ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 2.26
    200ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 2.21
    300ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 2.09
    400ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 2.19
    ì—í¬í¬ 1
    0ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.90
    100ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.90
    200ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.83
    300ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.71
    400ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.80
    ì—í¬í¬ 2
    0ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.58
    100ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.60
    200ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.51
    300ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.43
    400ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.49
    ì—í¬í¬ 3
    0ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.32
    100ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.36
    200ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.24
    300ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.21
    400ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.26
    ì—í¬í¬ 4
    0ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.13
    100ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.18
    200ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.04
    300ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.05
    400ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.10
    ì—í¬í¬ 5
    0ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.98
    100ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 1.04
    200ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.90
    300ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.93
    400ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.98
    ì—í¬í¬ 6
    0ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.87
    100ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.93
    200ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.79
    300ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.83
    400ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.89
    ì—í¬í¬ 7
    0ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.79
    100ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.84
    200ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.72
    300ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.76
    400ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.83
    ì—í¬í¬ 8
    0ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.73
    100ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.77
    200ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.65
    300ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.71
    400ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.77
    ì—í¬í¬ 9
    0ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.68
    100ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.72
    200ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.60
    300ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.66
    400ë²ˆì§¸ ë°°ì¹˜ ì†ì‹¤: 0.73
    

- ëª¨ë¸ í‰ê°€


```python
predictions = model(test_images)
predictions = predictions.numpy() # ë„˜íŒŒì´ ë°°ì—´ë¡œ ë°”ê¿ˆ
predicted_labels = np.argmax(predictions, axis=1)
matches = predicted_labels == test_labels
print(f"ì •í™•ë„: {matches.mean():.2f}")
```

    ì •í™•ë„: 0.82
    


```python

```
